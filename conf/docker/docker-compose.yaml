x-default-health-check: &default-health-check
  interval: ${HEALTH_CHECK_INTERVAL}
  timeout: ${HEALTH_CHECK_TIMEOUT}
  retries: ${HEALTH_CHECK_RETRIES}
  start_period: ${HEALTH_CHECK_START_PERIOD}

networks:
  mc-infra-connector-network:
  mc-infra-manager-network:
  mc-iam-manager-network:
  mc-cost-optimizer-network:
  mc-application-manager-network:
  mc-workflow-manager-network:
  mc-data-manager-network:
  mc-observability-network:
  mc-web-console-network:

volumes:
  grafana_shared_config:
  shared_logs:

services:
##### MC-INFRA-CONNECTOR #########################################################################################################################

  mc-infra-connector:
    image: cloudbaristaorg/cb-spider:0.11.16
    pull_policy: missing
    container_name: mc-infra-connector
    platform: linux/amd64
    networks:
      - mc-infra-connector-network
      - mc-web-console-network
    ports:
      - target: 1024
        published: 1024
        protocol: tcp
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - ./conf/mc-infra-connector/:/root/go/src/github.com/cloud-barista/cb-spider/conf/:ro
      - ./container-volume/mc-infra-connector/meta_db/:/root/go/src/github.com/cloud-barista/cb-spider/meta_db/
      - ./container-volume/mc-infra-connector/log/:/root/go/src/github.com/cloud-barista/cb-spider/log/
    environment:
      - PLUGIN_SW=OFF
      - SERVER_ADDRESS=0.0.0.0:1024
      # if you leave these values empty, REST Auth will be disabled.
      # - API_USERNAME=
      # - API_PASSWORD=
      - SPIDER_LOG_LEVEL=error
      - SPIDER_HISCALL_LOG_LEVEL=error
      - ID_TRANSFORM_MODE=OFF
      - ADMINWEB=OFF
    healthcheck: # for CB-Spider
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:1024/spider/readyz" ]
      <<: *default-health-check

##### MC-INFRA-MANAGER #########################################################################################################################

  mc-infra-manager:
    image: cloudbaristaorg/cb-tumblebug:0.11.16
    container_name: mc-infra-manager
    pull_policy: missing
    platform: linux/amd64
    networks:
      - mc-infra-connector-network
      - mc-infra-manager-network
      - mc-web-console-network
    ports:
      - target: 1323
        published: 1323
        protocol: tcp
    depends_on:
      mc-infra-manager-etcd:
        condition: service_started
      mc-infra-connector:
        condition: service_started
      mc-infra-manager-postgres:
        condition: service_healthy
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - ./conf/mc-infra-manager/:/app/conf/:ro
      - ./container-volume/mc-infra-manager/meta_db/:/app/meta_db/
      - ./container-volume/mc-infra-manager/log/:/app/log/
    environment:
      # - TB_ROOT_PATH=/app
      - TB_SPIDER_REST_URL=http://mc-infra-connector:1024/spider
      - TB_ETCD_ENDPOINTS=http://mc-infra-manager-etcd:2379
      - TB_TERRARIUM_REST_URL=http://mc-terrarium:8055/terrarium
      - TB_IAM_MANAGER_REST_URL=http://mc-iam-manager:5000
      # - TB_ETCD_AUTH_ENABLED=true
      # - TB_ETCD_USERNAME=default
      # - TB_ETCD_PASSWORD=default
      - TB_POSTGRES_ENDPOINT=mc-infra-manager-postgres:5432
      - TB_POSTGRES_DATABASE=tumblebug
      - TB_POSTGRES_USER=tumblebug
      - TB_POSTGRES_PASSWORD=tumblebug
      # - TB_ALLOW_ORIGINS=*
      # - TB_AUTH_ENABLED=true
      # - TB_API_USERNAME=default
      # - TB_API_PASSWORD=default
      # - TB_AUTOCONTROL_DURATION_MS=10000
      # - TB_SELF_ENDPOINT=localhost:1323
      # - TB_DRAGONFLY_REST_URL=http://cb-dragonfly:9090/dragonfly
      # - TB_DEFAULT_NAMESPACE=ns01
      # - TB_DEFAULT_CREDENTIALHOLDER=admin
      # - TB_LOGFILE_PATH=/app/log/tumblebug.log
      # - TB_LOGFILE_MAXSIZE=10
      # - TB_LOGFILE_MAXBACKUPS=3
      # - TB_LOGFILE_MAXAGE=30
      # - TB_LOGFILE_COMPRESS=false
      # - TB_LOGLEVEL=debug
      # - TB_LOGWRITER=both
      # - TB_NODE_ENV=development
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:1323/tumblebug/readyz" ]
      <<: *default-health-check

  mc-infra-manager-etcd:
    image: gcr.io/etcd-development/etcd:v3.5.14
    container_name: mc-infra-manager-etcd
    networks:
      - mc-infra-manager-network
    ports:
      - target: 2379
        published: 2379
        protocol: tcp
      - target: 2380
        published: 2380
        protocol: tcp
    volumes:
      - ./container-volume/mc-infra-manager/etcd/data:/etcd-data
    entrypoint: /usr/local/bin/etcd
    command:
      - --name
      - s1
      - --data-dir
      - /etcd-data
      - --listen-client-urls
      - http://0.0.0.0:2379
      - --advertise-client-urls
      - http://0.0.0.0:2379
      - --listen-peer-urls
      - http://0.0.0.0:2380
      - --initial-advertise-peer-urls
      - http://0.0.0.0:2380
      - --initial-cluster
      - s1=http://0.0.0.0:2380
      - --initial-cluster-token
      - tkn
      - --initial-cluster-state
      - new
      - --log-level
      - info
      - --logger
      - zap
      - --log-outputs
      - stderr
      - --auth-token
      - simple
    healthcheck:
      test: [ "CMD", "etcdctl", "endpoint", "health", "--endpoints=http://localhost:2379"]
      <<: *default-health-check


  # mc-infra-manager PostgreSQL
  # This is used for storing CB-Tumblebug Spec and Image.
  mc-infra-manager-postgres:
    image: postgres:16-alpine
    container_name: mc-infra-manager-postgres
    restart: always
    networks:
      - mc-infra-manager-network
      # # Enable external network for outbound access (not ideal for security)
      # - external_network
    ports:
      - 6432:5432
    volumes:
      - ./container-volume/mc-infra-manager/meta_db/postgres:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=tumblebug
      - POSTGRES_PASSWORD=tumblebug
      - POSTGRES_DB=tumblebug
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U cb_tumblebug"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s


##### MC-IAM-MANAGER #########################################################################################################################

  mc-iam-manager:
    container_name: mc-iam-manager
    # image: mc-iam-manager:latest
    image: cloudbaristaorg/mc-iam-manager:edge
    restart: unless-stopped
    networks:
      - mc-iam-manager-network
      - mc-infra-manager-network
      - mc-web-console-network
    ports:
      - target: ${MC_IAM_MANAGER_PORT}
        published: ${MC_IAM_MANAGER_PORT}
        protocol: tcp
    depends_on:
      - mc-iam-manager-db
      - mc-iam-manager-kc
    environment:
      DATABASE_URL: postgres://${MC_IAM_MANAGER_DATABASE_USER}:${MC_IAM_MANAGER_DATABASE_PASSWORD}@${MC_IAM_MANAGER_DATABASE_HOST}:5432/${MC_IAM_MANAGER_DATABASE_NAME}
      PORT: ${MC_IAM_MANAGER_PORT}
    env_file:
      - ./conf/mc-iam-manager/.env
    volumes:
      - ./tool/mcc:/app/tool/mcc
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://${MC_IAM_MANAGER_DOMAIN}:${MC_IAM_MANAGER_PORT}/readyz" ]
      <<: *default-health-check

  mc-iam-manager-db:
    container_name: mc-iam-manager-db
    image: postgres:14-alpine
    pull_policy: missing
    platform: linux/amd64
    restart: unless-stopped
    networks:
      - mc-iam-manager-network
    ports:
      - target: 5432
        published: 5432
        protocol: tcp
    volumes:
      - ./container-volume/mc-iam-manager/postgres/postgres_data:/var/lib/postgresql/data
      - ./conf/mc-iam-manager/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh:ro
    environment:
      POSTGRES_DB: ${MC_IAM_MANAGER_DATABASE_NAME}
      POSTGRES_USER: ${MC_IAM_MANAGER_DATABASE_USER}
      POSTGRES_PASSWORD: ${MC_IAM_MANAGER_DATABASE_PASSWORD}
    env_file:
      - ./conf/mc-iam-manager/.env
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${MC_IAM_MANAGER_DATABASE_USER} -d ${MC_IAM_MANAGER_DATABASE_NAME}" ]
      <<: *default-health-check
    command: >
      sh -c "chown -R 70:70 /var/lib/postgresql/data &&
            exec docker-entrypoint.sh postgres"

  mc-iam-manager-kc:
    container_name: mc-iam-manager-kc
    image: quay.io/keycloak/keycloak:24.0.1
    restart: unless-stopped
    networks:
      - mc-iam-manager-network
    ports:
      - target: 8080
        published: 8080
        protocol: tcp
    command:
      - start-dev
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://mc-iam-manager-db:5432/${MC_IAM_MANAGER_KEYCLOAK_DATABASE_NAME}
      KC_DB_USERNAME: ${MC_IAM_MANAGER_DATABASE_USER}
      KC_DB_PASSWORD: ${MC_IAM_MANAGER_DATABASE_PASSWORD}
      KC_HOSTNAME_PORT: 8080
      KC_HOSTNAME_STRICT: false
      KC_HOSTNAME_STRICT_HTTPS: false
      KC_HOSTNAME: localhost
      KEYCLOAK_ADMIN: ${MC_IAM_MANAGER_KEYCLOAK_ADMIN:-admin}
      KEYCLOAK_ADMIN_PASSWORD: ${MC_IAM_MANAGER_KEYCLOAK_ADMIN_PASSWORD:-admin_password}
      KC_HTTP_ENABLED: "true"
      KC_PROXY: edge
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - ./container-volume/mc-iam-manager/keycloak/data/:/opt/keycloak/data/
    env_file:
      - ./conf/mc-iam-manager/.env
    depends_on:
      - mc-iam-manager-db
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:8080/" ]
      # test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:8080/health/ready" ]
      <<: *default-health-check

  mc-iam-manager-nginx:
    image: nginx:1.25-alpine
    container_name: mc-iam-manager-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    environment:
      - DOMAIN_NAME=${MC_IAM_MANAGER_KEYCLOAK_DOMAIN}
    volumes:
      - ./container-volume/mc-iam-manager/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./container-volume/certs:/etc/nginx/certs:ro
      - ./container-volume/certbot/www:/var/www/certbot:ro
    depends_on:
      - mc-iam-manager-kc
    networks:
      - mc-iam-manager-network

  mc-iam-manager-post-initial:
    image: ubuntu:22.04
    container_name: mc-iam-manager-post-initial
    # restart: unless-stopped
    networks:
      - mc-iam-manager-network
    depends_on:
      mc-iam-manager:
        condition: service_started
      mc-iam-manager-db:
        condition: service_healthy
      mc-iam-manager-kc:
        condition: service_healthy
    env_file:
      - ./conf/mc-iam-manager/.env
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - ./conf/mc-iam-manager/:/app/mc-iam-manager/
    working_dir: /app/mc-iam-manager
    command: bash /app/mc-iam-manager/docker-post-init.sh


##### MC-COST-OPTIMIZER #########################################################################################################################

  mc-cost-optimizer-fe:
    restart: on-failure
    container_name: mc-cost-optimizer-fe
    image: cloudbaristaorg/mc-costopti-ui:0.4.0
    networks:
      - mc-cost-optimizer-network
      - mc-observability-network
      - mc-infra-manager-network
      - mc-web-console-network
    ports:
      - target: 80
        published: 7780
        protocol: tcp
    depends_on:
      - mc-cost-optimizer-be
    volumes:
      - ./tool/mcc:/app/tool/mcc
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:80" ]
      <<: *default-health-check

  mc-cost-optimizer-be:
    restart: on-failure
    container_name: mc-cost-optimizer-be
    image: cloudbaristaorg/mc-costopti-api:0.4.0
    networks:
      - mc-cost-optimizer-network
      - mc-observability-network
      - mc-infra-manager-network
      - mc-web-console-network
    ports:
      - target: 9090
        published: 9090
        protocol: tcp
    depends_on:
      - mc-cost-optimizer-db
    volumes:
      - ./tool/mcc:/app/tool/mcc
    environment:
      spring.datasource.hikari.cost.optimize.jdbc-url: ${CO_COST_DB_URL}
      spring.datasource.hikari.cost.optimize.username: ${CO_MYSQL_USER}
      spring.datasource.hikari.cost.optimize.password: ${CO_MYSQL_PASSWORD}
      tumblebug.url: http://mc-infra-manager:1323/tumblebug
      tumblebug.username: default
      tumblebug.password: default
      costopti.alarmservice.url: ${CO_ALARM_URL}
      costopti.assetcollector.url: ${CO_COST_ASSET_COLLECTOR_URL}
      costopti.costcollector.url: ${CO_COST_COLLECTOR_URL}
      costopti.costprocessor.url: ${CO_COST_PROCESSOR_URL}
      costopti.costselector.url: ${CO_COST_SELECTOR_URL}
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get",  "http://localhost:9090/api/costopti/be/readyz" ]
      <<: *default-health-check

  mc-cost-optimizer-cost-collector:
    restart: on-failure
    container_name: mc-cost-optimizer-cost-collector
    image: cloudbaristaorg/mc-costopti-costcollector:0.4.0
    networks:
      - mc-cost-optimizer-network
      - mc-observability-network
      - mc-infra-manager-network
      - mc-web-console-network
    ports:
      - target: 8081
        published: 8881
        protocol: tcp
    depends_on:
      - mc-cost-optimizer-db
    volumes:
      - ./tool/mcc:/app/tool/mcc
    environment:
      spring.datasource.hikari.batch.jdbc-url: ${CO_COST_DB_URL}
      spring.datasource.hikari.batch.username: ${CO_MYSQL_USER}
      spring.datasource.hikari.batch.password: ${CO_MYSQL_PASSWORD}
      unusedBatchCronSchedule: ${CO_COST_COLLECT_UNUSED_CRON_SCHEDULE}
      curBatchCronSchedule: ${CO_COST_COLLECT_CUR_CRON_SCHEDULE}
      aws.data.export.name: ${CO_AWS_CUR_EXPORT_NAME}
      aws.data.export.path.prefix: ${CO_AWS_CUR_EXPORT_PATH_PREFIX}
      AWS_ACCESS_KEY_ID: ${CO_AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${CO_AWS_SECRET_ACCESS_KEY}

  mc-cost-optimizer-cost-processor:
    restart: on-failure
    container_name: mc-cost-optimizer-cost-processor
    image: cloudbaristaorg/mc-costopti-costprocessor:0.4.0
    networks:
      - mc-cost-optimizer-network
      - mc-observability-network
      - mc-infra-manager-network
      - mc-web-console-network
    ports:
      - target: 8082
        published: 18082
        protocol: tcp
    depends_on:
      - mc-cost-optimizer-db
    volumes:
      - ./tool/mcc:/app/tool/mcc
    environment:
      spring.datasource.hikari.batch.jdbc-url: ${CO_COST_DB_URL}
      spring.datasource.hikari.batch.username: ${CO_MYSQL_USER}
      spring.datasource.hikari.batch.password: ${CO_MYSQL_PASSWORD}
      unusedProcessCronSchedule: ${CO_COST_PROCESS_UNUSED_CRON_SCHEDULE}
      abnormalProcessCronSchedule: ${CO_COST_PROCESS_ABNORMAL_CRON_SCHEDULE}
      cost.selector.url: ${CO_COST_SELECTOR_URL}
      opti.alarm.url: ${CO_ALARM_URL}

  mc-cost-optimizer-cost-selector:
    restart: on-failure
    container_name: mc-cost-optimizer-cost-selector
    image: cloudbaristaorg/mc-costopti-costselector:0.4.0
    networks:
      - mc-cost-optimizer-network
      - mc-observability-network
      - mc-infra-manager-network
      - mc-web-console-network
    ports:
      - target: 8083
        published: 8083
        protocol: tcp
    depends_on:
      - mc-cost-optimizer-db
    volumes:
      - ./tool/mcc:/app/tool/mcc
    environment:
      spring.datasource.hikari.cost.optimize.jdbc-url: ${CO_COST_DB_URL}
      spring.datasource.hikari.cost.optimize.username: ${CO_MYSQL_USER}
      spring.datasource.hikari.cost.optimize.password: ${CO_MYSQL_PASSWORD}
      opti.alarm.url: ${CO_ALARM_URL}

  mc-cost-optimizer-alarm-service:
    restart: on-failure
    container_name: mc-cost-optimizer-alarm-service
    image: cloudbaristaorg/mc-costopti-alarm:0.4.0
    networks:
      - mc-cost-optimizer-network
      - mc-observability-network
      - mc-infra-manager-network
      - mc-web-console-network
    ports:
      - target: 9000
        published: 9000
        protocol: tcp
    depends_on:
      - mc-cost-optimizer-db
    volumes:
      - ./tool/mcc:/app/tool/mcc
    environment:
      spring.datasource.hikari.slack.jdbc-url: ${CO_SLACK_DB_URL}
      spring.datasource.hikari.slack.username: ${CO_MYSQL_USER}
      spring.datasource.hikari.slack.password: ${CO_MYSQL_PASSWORD}
      spring.datasource.hikari.mailing.jdbc-url: ${CO_MAIL_DB_URL}
      spring.datasource.hikari.mailing.username: ${CO_MYSQL_USER}
      spring.datasource.hikari.mailing.password: ${CO_MYSQL_PASSWORD}
      spring.datasource.hikari.history.jdbc-url: ${CO_COST_DB_URL}
      spring.datasource.hikari.history.username: ${CO_MYSQL_USER}
      spring.datasource.hikari.history.password: ${CO_MYSQL_PASSWORD}

  mc-cost-optimizer-asset-collector:
    restart: on-failure
    container_name: mc-cost-optimizer-asset-collector
    image: cloudbaristaorg/mc-costopti-assetcollector:0.4.0
    networks:
      - mc-cost-optimizer-network
      - mc-observability-network
      - mc-infra-manager-network
      - mc-web-console-network
    ports:
      - target: 8091
        published: 8091
        protocol: tcp
    depends_on:
      - mc-cost-optimizer-db
    volumes:
      - ./tool/mcc:/app/tool/mcc
    environment:
      spring.datasource.hikari.batch.jdbc-url: ${CO_COST_DB_URL}
      spring.datasource.hikari.batch.username: ${CO_MYSQL_USER}
      spring.datasource.hikari.batch.password: ${CO_MYSQL_PASSWORD}
      costopti.be.url: ${CO_API_URL}
      asset.collect.url: ${CO_ASSET_MONITORING_SERVER}
      assetCollectBatchCronSchedule: ${CO_ASSET_COLLECT_BATCH_CRON_SCHEDULE}

  mc-cost-optimizer-init-volume:
    image: busybox:stable
    container_name: mc-cost-optimizer-init-volume
    command: ["sh", "-c", "chown -R 999:999 /var/lib/mysql"]
    volumes:
      - ./container-volume/mc-cost-optimizer/mysql/:/var/lib/mysql
    user: root
    networks:
      - mc-cost-optimizer-network
    init: true
    restart: "no"

  mc-cost-optimizer-db:
    image: mariadb:latest
    container_name: mc-cost-optimizer-db
    depends_on:
      mc-cost-optimizer-init-volume:
        condition: service_completed_successfully
    environment:
      - ALLOW_EMPTY_PASSWORD=no
      - MYSQL_ROOT_PASSWORD=${CO_MYSQL_ROOT_PASSWORD}
      - MYSQL_USER=${CO_MYSQL_USER}
      - MYSQL_PASSWORD=${CO_MYSQL_PASSWORD}
    command:
      - --skip-character-set-client-handshake
    ports:
      - target: 3306
        published: 3307
        protocol: tcp
    volumes:
      - ./conf/mc-cost-optimizer/init/:/docker-entrypoint-initdb.d
      - ./container-volume/mc-cost-optimizer/mysql/:/var/lib/mysql/
    networks:
      - mc-cost-optimizer-network
      - mc-web-console-network
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--su-mysql", "--connect", "--innodb_initialized"]
      <<: *default-health-check

##### MC-APPLICATION-MANAGER #########################################################################################################################

  mc-application-manager-sonatype-nexus:
    image: sonatype/nexus3:latest
    container_name: mc-application-manager-sonatype-nexus
    platform: linux/amd64
    networks:
      - mc-application-manager-network
      - mc-web-console-network
    ports:
      - target: 8081
        published: 8081
        protocol: tcp
      - target: 5000
        published: 5500
        protocol: tcp
    volumes:
      - ./container-volume/mc-application-manager/nexus/:/nexus-data:rw
    environment:
      NEXUS_SECURITY_RANDOMPASSWORD: 'false'
      NEXUS_SECURITY_INITIAL_PASSWORD: 123456 # Please CHANGE ME
    user: root
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      <<: *default-health-check

  mc-application-manager:
    image: cloudbaristaorg/mc-application-manager:0.4.2
    container_name: mc-application-manager
    networks:
      - mc-application-manager-network
      - mc-web-console-network
    ports:
      - target: 18084
        published: 18084
        protocol: tcp
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - ./container-volume/mc-application-manager/nexus/:/nexus-data:rw
    user: root
    environment:
      - DDL_AUTO=update
      - DB_USER=application # Please CHANGE ME
      - DB_PASS=application!23 # Please CHANGE ME
      - SQL_DATA_INIT=never
      - TUMBLEBUG_URL=mc-infra-manager
      - TUMBLEBUG_PORT=1323
      - TUMBLEBUG_ID=default
      - TUMBLEBUG_PASSWORD=default
      - SPIDER_URL=mc-infra-connector
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:18084/readyz" ]
      <<: *default-health-check


##### MC-WORKFLOW-MANAGER #########################################################################################################################

  mc-workflow-manager-jenkins:
    image: jenkins/jenkins:2.462.3-lts
    container_name: mc-workflow-manager-jenkins
    platform: linux/amd64
    networks:
      - mc-workflow-manager-network
      - mc-web-console-network
      - mc-infra-manager-network
    ports:
      - target: 8080
        published: 9880
        protocol: tcp
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /usr/bin/docker:/usr/bin/docker
      - ./container-volume/mc-workflow-manager/jenkins/:/var/jenkins_home:rw
      - ./tool/init.groovy.d:/usr/share/jenkins/ref/init.groovy.d
    environment:
      JENKINS_USERNAME: admin
      JENKINS_PASSWORD: 123456 # Please CHANGE ME
      JAVA_OPTS: -Djenkins.install.runSetupWizard=false
    user: root
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/login"]
      <<: *default-health-check


  mc-workflow-manager:
    image: cloudbaristaorg/mc-workflow-manager:0.4.2
    container_name: mc-workflow-manager
    platform: linux/amd64
    networks:
      - mc-workflow-manager-network
      - mc-web-console-network
    ports:
      - target: 18083
        published: 18083
        protocol: tcp
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - ./conf/mc-workflow-manager/document/:/document/
    environment:
      - DB_INIT_YN=create
      - DB_ID=workflow # Please CHANGE ME
      - DB_PW=workflow!23 # Please CHANGE ME
      - SQL_DATA_INIT=always
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:18083/readyz" ]
      <<: *default-health-check


##### MC-DATA-MANAGER #########################################################################################################################

  mc-data-manager-init-volumes:
    image: busybox:stable
    container_name: mc-data-manager-init-volumes
    command: ["sh", "-c", "chown -R ${UID:-0}:${GID:-0} /app/data"]
    volumes:
      - ./conf/mc-data-manager/data:/app/data/
    user: root
    env_file:
      - ./conf/mc-data-manager/.env
    init: true
    networks:
      - mc-data-manager-network

  mc-data-manager:
    image: cloudbaristaorg/mc-data-manager:0.5.0
    container_name: mc-data-manager
    depends_on:
      mc-data-manager-init-volumes:
        condition: service_completed_successfully
      mc-data-manager-db:
        condition: service_healthy
    build:
      context: .
      dockerfile: Dockerfile
      args:
        UID: ${UID:-0}
        GID: ${GID:-0}
        USER: ${USER_NAME:-root}
        GROUP: ${GROUP_NAME:-root}
        WEB_DIR : ${WEB_DIR:-/web}
        APP_HOME: /app
    tty: true
    ports:
      - "3300:3300"
    restart : always
    volumes:
      - ./data:/app/data/ # container-volume
      - ./scripts:/app/scripts/ # container-volume
      - /etc/localtime:/etc/localtime:ro # timezone
    environment:
      - TUMBLEBUG_URL=http://mc-infra-manager:1323
    env_file:
      - ./conf/mc-data-manager/.env
    # Health check configuration
    # OK [ 2xx ,3xx}, ERR [4xx,5xx,...etc]
    networks:
      - mc-data-manager-network
      - mc-web-console-network
      - mc-infra-manager-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3300/readyZ"]
      interval: 30s
      timeout: 10s
      retries: 3
      <<: *default-health-check

  mc-data-manager-db:
    image: mariadb:latest
    container_name: mc-data-manager-db
    command:
      - --skip-character-set-client-handshake
    volumes:
      - ./container-volume/mc-data-manager/mariadb:/var/lib/mysql
    ports:
      - target: 3306
        published: 3301
        protocol: tcp
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--su-mysql", "--connect", "--innodb_initialized"]
      interval: 30s
      timeout: 10s
      retries: 3
      <<: *default-health-check
    networks:
      - mc-data-manager-network
    environment:
      - MYSQL_DATABASE=mcmp
      - MYSQL_USER=mcmp
      - MYSQL_PASSWORD=mcmp
      - MYSQL_ROOT_PASSWORD=mcmp


##### MC-WEB-CONSOLE #########################################################################################################################


  mc-web-console-db:
    image: postgres:14-alpine
    container_name: mc-web-console-db
    restart: unless-stopped
    volumes:
      - ./container-volume/mc-web-console/postgres/postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: ${CONSOLE_POSTGRES_DB}
      POSTGRES_USER: ${CONSOLE_POSTGRES_USER}
      POSTGRES_PASSWORD: ${CONSOLE_POSTGRES_PASSWORD}
    networks:
      - mc-web-console-network
    pull_policy: missing
    platform: linux/amd64
    ports:
      - target: 5432
        published: 15432
        protocol: tcp
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${CONSOLE_POSTGRES_USER}" ]
      <<: *default-health-check
    command: >
      sh -c "chown -R 70:70 /var/lib/postgresql/data &&
            exec docker-entrypoint.sh postgres"

  mc-web-console-api:
    #image: cloudbaristaorg/mc-web-console-api:0.4.11
    image: cloudbaristaorg/mc-web-console-api:edge
    container_name: mc-web-console-api
    platform: linux/amd64
    restart: unless-stopped
    depends_on:
      - mc-web-console-db
      - mc-infra-connector
      - mc-infra-manager
      - mc-iam-manager
      - mc-observability-manager
      - mc-workflow-manager
      - mc-data-manager
      - mc-application-manager
      - mc-cost-optimizer-be
    ports:
      - target: 3000
        published: 3000
        protocol: tcp
    networks:
      - mc-web-console-network
      - mc-iam-manager-network
    environment:
      GO_ENV: development
      GODEBUG: netdns=go
      API_ADDR: "0.0.0.0"
      API_PORT: "3000"
      DATABASE_URL: postgres://${CONSOLE_POSTGRES_USER}:${CONSOLE_POSTGRES_PASSWORD}@mc-web-console-db:5432/${CONSOLE_POSTGRES_DB}
      MCIAM_USE: true
      MCIAM_TICKET_USE: false
      IFRAME_TARGET_IS_HOST: true
      MC_IAM_MANAGER_PORT: ${MC_IAM_MANAGER_PORT}
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - ./conf/mc-web-console/api/conf/:/conf/
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:3000/readyz" ]
      <<: *default-health-check

  mc-web-console-front:
    #image: cloudbaristaorg/mc-web-console-front:0.4.11
    image: cloudbaristaorg/mc-web-console-front:edge
    container_name: mc-web-console-front
    platform: linux/amd64
    restart: unless-stopped
    depends_on:
      - mc-web-console-api
    networks:
      - mc-web-console-network
      - mc-iam-manager-network
    ports:
      - target: 3001
        published: 3001
        protocol: tcp
    environment:
      API_ADDR: mc-web-console-api
      API_PORT: 3000
      FRONT_ADDR: 0.0.0.0
      FRONT_PORT: 3001
    volumes:
      - ./tool/mcc:/app/tool/mcc
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:3001" ]
      <<: *default-health-check
##### MC-OBSERVABILITY #########################################################################################################################
  mc-observability-manager:
    image: cloudbaristaorg/mc-observability:0.4.4
    container_name: mc-observability-manager
    restart: on-failure
    networks:
      - mc-observability-network
      - mc-cost-optimizer-network
      - mc-infra-manager-network
      - mc-infra-connector-network
      - mc-web-console-network
    ports:
      - target: 18080
        published: 18080
        protocol: tcp
    depends_on:
      mc-observability-grafana:
        condition: service_healthy
      mc-observability-infra:
        condition: service_healthy
      mc-observability-influx:
        condition: service_healthy
      mc-observability-influx-2:
        condition: service_healthy
      mc-observability-loki:
        condition: service_healthy
      mc-observability-maria:
        condition: service_healthy
      mc-observability-rabbitmq:
        condition: service_healthy
      mc-observability-tempo:
        condition: service_healthy
    env_file:
      - ./conf/mc-observability/manager/.env
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - shared_logs:/applog/mc-o11y-manager:rw
      - grafana_shared_config:/grafana_config:ro
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:18080/api/docs" ]
      <<: *default-health-check

  mc-observability-infra:
    image: cloudbaristaorg/mc-observability-infra:0.4.3
    container_name: mc-observability-infra
    restart: on-failure
    networks:
      - mc-observability-network
    ports:
      - target: 3000
        published: 33000
        protocol: tcp
    depends_on:
      mc-observability-maria:
        condition: service_healthy
    env_file:
      - ./conf/mc-observability/mc-observability-infra/.env
    volumes:
      - ./conf/mc-observability/mc-observability-infra/ansible:/ansible
    healthcheck:
      start_period: 120s
      test: 'curl -s http://127.0.0.1:3000/api/ping ; STATUS=`echo $$?` ; if [ $$STATUS != 0 ] && [ $$STATUS != 52 ]; then exit 1 ; fi'
      interval: 10s
      timeout: 10s
      retries: 30

  mc-observability-rabbitmq-init-volumes:
    image: busybox:stable
    container_name: mc-observability-rabbitmq-init-volumes
    restart: no
    networks:
      - mc-observability-network
    command:
      - sh
      - -c
      - |
        set -eux && \
        mkdir -p /var/lib/rabbitmq /var/log/rabbitmq && \
        if [ ! -s /var/lib/rabbitmq/.erlang.cookie ] || \
           [ "$(wc -c < /var/lib/rabbitmq/.erlang.cookie)" -lt 20 ]; then
          umask 177
          tr -dc 'A-Za-z0-9' </dev/urandom | head -c 48 > /var/lib/rabbitmq/.erlang.cookie
        fi && \
        chmod 600 /var/lib/rabbitmq/.erlang.cookie && \
        chown -R 132:141 /var/lib/rabbitmq /var/log/rabbitmq
    volumes:
      - ./container-volume/mc-observability/rabbitmq/data:/var/lib/rabbitmq
    user: root
    init: true

  mc-observability-rabbitmq:
    image: rabbitmq:4.1.4-management-alpine
    container_name: mc-observability-rabbitmq
    restart: on-failure
    networks:
      - mc-observability-network
    ports:
      - target: 5672
        published: 5672
        protocol: tcp
      - target: 1883
        published: 1883
        protocol: tcp
      - target: 15672
        published: 15672
        protocol: tcp
      # - target: 8883
      #   published: 8883
      #   protocol: tcp
      # - target: 15675
      #   published: 15675
      #   protocol: tcp
    depends_on:
      mc-observability-rabbitmq-init-volumes:
        condition: service_completed_successfully
    environment:
      - RABBITMQ_ENABLED_PLUGINS_FILE=/etc/rabbitmq/enabled_plugins
      - RABBITMQ_LOAD_DEFINITIONS=/etc/rabbitmq/definitions.json
    volumes:
      - ./conf/mc-observability/rabbitmq:/etc/rabbitmq
      - ./container-volume/mc-observability/rabbitmq/data:/var/lib/rabbitmq
    healthcheck:
      start_period: 120s
      test: rabbitmq-diagnostics -q ping
      interval: 10s
      timeout: 10s
      retries: 30

  mc-observability-maria:
    image: mariadb:10.11.11
    container_name: mc-observability-maria
    restart: on-failure
    networks:
      - mc-observability-network
    ports:
      - target: 3306
        published: 3306
        protocol: tcp
    environment:
      - TZ="Asia/Seoul"
      - ALLOW_EMPTY_PASSWORD=no
      - MARIADB_ROOT_PASSWORD=qwe1212!Q
      - MARIADB_USER=mc-agent
      - MARIADB_DATABASE=mc_observability
      - MARIADB_PASSWORD=mc-agent
    volumes:
      - ./conf/mc-observability/mariadb/maria_init.sql:/docker-entrypoint-initdb.d/maria_init.sql
      - ./conf/mc-observability/mariadb/99-max-connections.cnf:/etc/mysql/mariadb.conf.d/99-max-connections.cnf
      - ./container-volume/mc-observability/maria/mysql/conf.d:/etc/mysql/conf.d:ro
      - ./container-volume/mc-observability/maria/mysql:/var/lib/mysql
      - ./container-volume/mc-observability/maria/log:/var/log/maria
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect"]
      <<: *default-health-check

  mc-observability-influx-init-volumes:
    image: busybox:stable
    container_name: mc-observability-influx-init-volumes
    restart: no
    networks:
      - mc-observability-network
    command: ["sh", "-c", "chown -R 1500:1500 /var/lib/influxdb"]
    volumes:
      - ./container-volume/mc-observability/influxdb/influxdb_data:/var/lib/influxdb
    user: root
    init: true

  mc-observability-influx:
    image: cloudbaristaorg/mc-observability-influx:0.4.3
    container_name: mc-observability-influx
    restart: on-failure
    networks:
      - mc-observability-network
    ports:
      - target: 8086
        published: 8086
        protocol: tcp
    depends_on:
      mc-observability-influx-init-volumes:
        condition: service_completed_successfully
    environment:
      - INFLUXDB_HTTP_AUTH_ENABLED=true
      - INFLUXDB_ADMIN_USER=mc-agent
      - INFLUXDB_ADMIN_PASSWORD=mc-agent
      - INFLUXDB_DB="mc-observability"
    volumes:
      - ./conf/mc-observability/influxdb/influxdb_init:/docker-entrypoint-initdb.d
      - ./container-volume/mc-observability/influxdb/influxdb_data:/var/lib/influxdb
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8086/health"]
      <<: *default-health-check

  mc-observability-influx-2-init-volumes:
    image: busybox:stable
    container_name: mc-observability-influx-2-init-volumes
    restart: no
    command: ["sh", "-c", "chown -R 1500:1500 /var/lib/influxdb"]
    volumes:
      - ./container-volume/mc-observability/influxdb/influxdb2_data:/var/lib/influxdb
    user: root
    init: true
    networks:
      - mc-observability-network

  mc-observability-influx-2:
    image: cloudbaristaorg/mc-observability-influx:0.4.3
    container_name: mc-observability-influx-2
    restart: on-failure
    networks:
      - mc-observability-network
    ports:
      - target: 8086
        published: 8087
        protocol: tcp
    depends_on:
      mc-observability-influx-init-volumes:
        condition: service_completed_successfully
    environment:
      - INFLUXDB_HTTP_AUTH_ENABLED=true
      - INFLUXDB_ADMIN_USER=mc-agent
      - INFLUXDB_ADMIN_PASSWORD=mc-agent
      - INFLUXDB_DB="mc-observability"
    volumes:
      - ./conf/mc-observability/influxdb/influxdb_init:/docker-entrypoint-initdb.d
      - ./container-volume/mc-observability/influxdb/influxdb2_data:/var/lib/influxdb
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8086/health"]
      <<: *default-health-check

##### Remove annotations if data inquiry is required with chronograf during development phase #####
#  mc-observability-chronograf:
#    image: chronograf:1.9.4
#    container_name: mc-observability-chronograf
#    networks:
#      - mc-web-console-network
#      - mc-observability-network
#    ports:
#      - target: 8888
#        published: 8888
#        protocol: tcp
#    volumes:
#      - ./container-volume/mc-observability/chronograf/chronograf_data:/var/lib/chronograf
#    healthcheck:
#      test: ["CMD", "curl", "-f", "http://localhost:8888/chronograf/v1/health"]
#      <<: *default-health-check

  mc-observability-minio:
    image: cloudbaristaorg/mc-observability-minio:0.4.3
    container_name: mc-observability-minio
    restart: on-failure
    networks:
      - mc-observability-network
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=mc-agent
    volumes:
      - ./container-volume/mc-observability/minio/minio_data:/minio/data
    healthcheck:
      start_period: 120s
      test: 'curl -s http://127.0.0.1:9001/api/docs ; STATUS=`echo $$?` ; if [ $$STATUS != 0 ] && [ $$STATUS != 52 ]; then exit 1 ; fi'
      interval: 10s
      timeout: 10s
      retries: 30

  mc-observability-loki-init-volumes:
    image: busybox:stable
    container_name: mc-observability-loki-init-volumes
    restart: no
    command: ["sh", "-c", "chown -R 10001:10001 /loki"]
    volumes:
      - ./container-volume/mc-observability/loki/loki_data:/loki
    user: root
    init: true
    networks:
      - mc-observability-network

  mc-observability-loki:
    image: grafana/loki:3.4.2
    container_name: mc-observability-loki
    restart: on-failure
    networks:
      - mc-observability-network
    ports:
      - target: 3100
        published: 3100
        protocol: tcp
    depends_on:
      mc-observability-loki-init-volumes:
        condition: service_completed_successfully
      mc-observability-minio:
        condition: service_healthy
    environment:
      - TZ=Asia/Seoul
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ./conf/mc-observability/loki/config.yml:/etc/loki/local-config.yaml
      - ./container-volume/mc-observability/loki/loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      start_period: 120s
      test: 'wget --quiet --spider http://127.0.0.1:3100/metrics || exit 1'
      interval: 10s
      timeout: 10s
      retries: 30

  mc-observability-tempo-init-volumes:
    image: busybox:stable
    container_name: mc-observability-tempo-init-volumes
    restart: no
    networks:
      - mc-observability-network
    command: ["sh", "-c", "chown -R 10001:10001 /etc/tempo"]
    volumes:
      - ./container-volume/mc-observability/tempo/tempo_data:/etc/tempo
    user: root
    init: true

  mc-observability-tempo:
    image: grafana/tempo:2.8.2
    container_name: mc-observability-tempo
    restart: always
    networks:
      - mc-observability-network
    ports:
      - target: 3200
        published: 3200
        protocol: tcp
      - target: 4317
        published: 4317
        protocol: tcp
      - target: 4318
        published: 4318
        protocol: tcp
    depends_on:
      mc-observability-tempo-init-volumes:
        condition: service_completed_successfully
      mc-observability-minio:
        condition: service_healthy
    environment:
      - TZ=Asia/Seoul
    command: [ "-config.file=/etc/tempo-config.yaml" ]
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - /etc/localtime:/etc/localtime:ro
      - ./conf/mc-observability/tempo/config.yaml:/etc/tempo-config.yaml
      - ./container-volume/mc-observability/tempo/tempo_data:/etc/tempo
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://127.0.0.1:3200/ready" ]
      <<: *default-health-check

  mc-observability-grafana-init-volumes:
    image: busybox:stable
    container_name: mc-observability-grafana-init-volumes
    restart: no
    networks:
      - mc-observability-network
    command: ["sh", "-c", "chown -R 472:472 /var/lib/grafana && chown -R 472:472 /var/log/grafana && chown -R 472:472 /grafana_config"]
    volumes:
      - ./container-volume/mc-observability/grafana/grafana_data/data:/var/lib/grafana
      - ./container-volume/mc-observability/grafana/grafana_data/log:/var/log/grafana
      - grafana_shared_config:/grafana_config
    user: root
    init: true

  mc-observability-grafana:
    image: cloudbaristaorg/mc-observability-grafana:0.4.3
    container_name: mc-observability-grafana
    restart: on-failure
    networks:
      - mc-observability-network
    ports:
      - target: 3000
        published: 33001
        protocol: tcp
    depends_on:
      mc-observability-grafana-init-volumes:
        condition: service_completed_successfully
    env_file:
      - ./conf/mc-observability/grafana/.env
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ./conf/mc-observability/grafana:/etc/grafana
      - ./container-volume/mc-observability/grafana/grafana_data/data:/var/lib/grafana
      - ./container-volume/mc-observability/grafana/grafana_data/log:/var/log/grafana
      - grafana_shared_config:/grafana_config
    healthcheck:
      start_period: 120s
      test: |
        curl -s http://127.0.0.1:3000
        STATUS=`echo $$?`
        if [ $$STATUS != 0 ] && [ $$STATUS != 52 ]; then
          exit 1
        fi
        curl -k -s https://127.0.0.1:8443
        STATUS=`echo $$?`
        if [ $$STATUS != 0 ] && [ $$STATUS != 52 ]; then
          exit 1
        fi
      interval: 10s
      timeout: 10s
      retries: 30

  mc-observability-insight:
    image: cloudbaristaorg/mc-observability-insight:0.4.3
    container_name: mc-observability-insight
    networks:
      - mc-observability-network
    ports:
      - target: 9001
        published: 9001
        protocol: tcp
    depends_on:
      mc-observability-maria:
        condition: service_healthy
    environment:
      - TZ=Asia/Seoul
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-}
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - shared_logs:/mc-insight/log:rw
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:9001/readyz" ]
      <<: *default-health-check

  mc-observability-insight-scheduler:
    image: cloudbaristaorg/mc-observability-insight-scheduler:0.4.3
    container_name: mc-observability-insight-scheduler
    networks:
      - mc-observability-network
    ports:
      - target: 9002
        published: 9002
        protocol: tcp
    depends_on:
      mc-observability-maria:
        condition: service_started
    environment:
      - TZ=Asia/Seoul
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+mysqldb://mc-agent:mc-agent@mc-observability-maria:3306/mc_airflow
      - AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - /etc/localtime:/etc/localtime:ro
      - ./container-volume/mc-observability/scheduler/logs:/usr/local/airflow/logs
    command: >
      /bin/bash -c "
        # Wait for MySQL
        sleep 10

        # Clean up pid
        rm -f airflow-webserver.pid

        # Set up metadata database
        airflow db init

        # Create default user
        airflow users create --username admin --password admin --email admin@test.com --firstname admin --lastname admin --role Admin

        # Import variables & Make connections
        # airflow variables import -a overwrite /usr/local/airflow/airflow_variables.json
        airflow variables set --description 'O11Y Manger API BASE URL' API_BASE_URL http://mc-observability-manager:18080/api/o11y

        airflow connections add --conn-type http --conn-host mc-observability-insight --conn-schema http --conn-port 9001 api_base_url
        airflow connections add --conn-type mysql --conn-host mc-observability-maria --conn-schema mc_observability --conn-login mc-agent --conn-password mc-agent --conn-port 3306 mcmp_db
        airflow connections add --conn-type http --conn-host mc-observability-manager --conn-port 18080 o11y-manager
        airflow connections add --conn-type influxdb --conn-host mc-observability-influx --conn-port 8086 --conn-schema downsampling --conn-login mc-agent --conn-password mc-agent influxdb

        # Reload & Run dags
        airflow dags reserialize
        airflow dags unpause anomaly_detection
        airflow dags unpause down_sampling

        # Start airflow
        airflow scheduler & airflow webserver -p 9002

        # Keep the server on no matter what
        sleep infinity
            "
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://localhost:9002/health" ]
      <<: *default-health-check

  mc-observability-log-collector:
    image: fluent/fluent-bit:3.2.4
    container_name: mc-observability-log-collector
    restart: on-failure
    networks:
      - mc-observability-network
    depends_on:
      mc-observability-loki:
        condition: service_healthy
    environment:
      - TZ=Asia/Seoul
      - LOKI_HOST=mc-observability-loki
      - LOKI_PORT=3100
    volumes:
      - ./tool/mcc:/app/tool/mcc
      - ./conf/mc-observability/log-collector:/fluent-bit/etc:ro
      - shared_logs:/logs:ro
    command:
      - /fluent-bit/bin/fluent-bit
      - -c
      - /fluent-bit/etc/fluent-bit.conf
    healthcheck:
      test: [ "CMD", "/app/tool/mcc", "rest", "get", "http://127.0.0.1:2020/api/v1/health" ]
      <<: *default-health-check

##### Remove annotations when using LLM-based log and alarm analysis #####
#  mc-observability-mcp-grafana:
#    image: mcp/grafana:latest
#    container_name: mc-observability-mcp-grafana
#    restart: on-failure
#    networks:
#      - mc-observability-network
#    ports:
#      - target: 8000
#        published: 8000
#        protocol: tcp
#    environment:
#      - GRAFANA_URL=http://mc-observability-grafana:3000
#    entrypoint: ["/bin/bash", "/app/docker-entrypoint.sh"]
#    command: ["/bin/bash", "-c", "chmod +x /app/docker-entrypoint.sh"]
#    volumes:
#      - ./conf/mc-observability/mcp-grafana/docker-entrypoint.sh:/app/docker-entrypoint.sh:ro
#      - grafana_shared_config:/grafana_config
#    tty: true
#    stdin_open: true
#
#  mc-observability-mcp-mariadb:
#    image: cloudbaristaorg/mc-observability-mcp-mariadb:0.4.3
#    container_name: mc-observability-mcp-mariadb
#    restart: on-failure
#    networks:
#      - mc-observability-network
#    ports:
#      - target: 8001
#        published: 8001
#        protocol: tcp
#    depends_on:
#      mc-observability-maria:
#        condition: service_healthy
#    environment:
#      - TZ=Asia/Seoul
#      - DB_HOST=mc-observability-maria
#      - DB_PORT=3306
#      - DB_USER=mc-agent
#      - DB_PASSWORD=mc-agent
#      - DB_NAME=mc_observability
#      - MCP_READ_ONLY=true
#      - MCP_MAX_POOL_SIZE=10
#      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
#
#  mc-observability-mcp-influx:
#    image: cloudbaristaorg/mc-observability-mcp-influxdb:0.4.3
#    container_name: mc-observability-mcp-influx
#    restart: always
#    networks:
#      - mc-observability-network
#    ports:
#      - target: 8002
#        published: 8002
#        protocol: tcp
#    depends_on:
#      mc-observability-influx:
#        condition: service_healthy
#    environment:
#      - TZ=Asia/Seoul
#      - INFLUXDB_URL=http://mc-observability-influx:8086
#      - INFLUXDB_USER=mc-agent
#      - INFLUXDB_PASSWORD=mc-agent
#      - INFLUXDB_DATABASE=mc-observability
